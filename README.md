# Python-ETL-pipeline-using-Airflow-on-AWS
This project demonstrates how to build and automate an ETL pipeline written in Python and schedule it using open source Apache Airflow orchestration tool on AWS EC2 instance.

# Project Goals 

1. Data Ingestion - Create a data ingestion pipeline to extract data from OpenWeather API.
2. Data Storage - Create a data storage repository using AWS S3 buckets.
3. Data Transformation - Create ETL job to extract the data, do simple transformations and load the clean data using Airflow.
4. Data Pipeline - Create a data pipeline written in Python that extracts data from API calls and store it in AWS S3 buckets.
5. Pipeline Automation - Create scheduling service using Apace Airflow to trigger the data pipeline and automate the process.


# Data Architecture

The architecture (Data flow) used in this project uses different Open source and cloud components as described below:

<p align="center">
  <img width="950" height="550" src="https://github.com/chayansraj/Python-ETL-pipeline-using-Airflow-on-AWS/assets/22219089/a63db442-0e05-48e9-9c03-27d808053c09">
  <h6 align = "center" > Source: Author </h6>
</p>


TBC...
